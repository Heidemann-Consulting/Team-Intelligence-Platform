# HORIZON Protocol Concept & Design Guidelines

**Version:** 1.0
**Date:** May 18, 2025
**Author:** Heidemann Consulting
**License:** Apache 2.0

**Introduction & Conceptual Framework:** The HORIZON Protocol is a next-generation AI co-management framework designed for small teams (1–9 members) using TIP (Team Intelligence Platform). It positions a local LLM as an integrated "team member" that shares in day-to-day management tasks. The goal is to dramatically boost strategic memory, team clarity, workflow automation, content consistency, and decision support, all while capping each person’s input to ≤30 minutes per day. HORIZON builds on proven AI integration principles: preserving organizational knowledge (so nothing is ever lost), augmenting human capacity (the AI handles rote synthesis and analysis), and enforcing consistency (through templates and workflows) – ultimately delivering high ROI in terms of saved time and improved output quality. In essence, HORIZON treats the AI as a diligent co-manager that works alongside humans to ensure nothing falls through the cracks, decisions are well-informed, and outputs align with the team’s standards.

**Guiding Principles:** The HORIZON Protocol adheres to key principles that ensure its effectiveness and alignment with TIP benefits:

* **Collective Strategic Memory:** All critical team knowledge – vision, values, goals, decisions, and context – is continuously captured and preserved in TIP Documents. This ensures the team **never starts from zero**, and the AI can leverage a rich memory bank for context. Institutional knowledge stays put even when individuals change roles, fostering a true collective intelligence.
* **AI-Boosted Capacity & Clarity:** The model serves as an always-on collaborator that extends team capacity without additional headcount. It transforms fragmented inputs (notes, ideas, data) into coherent outputs (plans, summaries, drafts) at speed. Routine tasks that would normally sap human time (summarizing meetings, drafting plans, checking consistency) are automated. This not only saves time but also multiplies clarity – the AI synthesizes scattered information into focused insights, giving the team a clarity and speed edge in daily operations.
* **Process & Workflow Optimization:** HORIZON emphasizes building repeatable workflows and templates for everything from daily logs to strategic reviews. Once a workflow or template is created, it can be reused continuously (“**build once, benefit daily**” approach). Workflows are designed to fit within the LLM’s context window and token limits, often by using summarization or focusing on deltas (new changes) to keep prompts efficient. By having the AI handle predictable parts of each workflow, the team can scale its output without burnout or wasted effort.
* **Content Consistency & Brand Voice:** Document templates encode the team’s tone, style, and key messaging, effectively making the AI a **brand guardian**. Whether drafting an email or a project update, the AI uses templates that maintain consistent formatting and language. This prevents drift in style or standards over time. The AI also cross-references values and guidelines in the knowledge base to ensure outputs adhere to the team’s principles and quality standards (for instance, decision documents automatically reference the team’s defined criteria and ethical guidelines).
* **Decision Support & Alignment:** HORIZON workflows actively support better decision-making. By having the AI recall relevant context (past decisions, strategic goals, real-time data) and present pros/cons or recommendations, the team gains a “clarity advantage” in complex situations. All suggestions are traceable back to the source documents (thanks to TIP’s versioned memory), ensuring transparency. The process also enforces alignment with defined goals and values – e.g., a major decision proposal is generated only after cross-checking against the North Star vision and current objectives, helping the team stay true to its compass.
* **Minimal Overhead, Maximum ROI:** Perhaps most critically, the protocol is engineered for efficiency. Human time investment is capped at ~30 minutes/day each by strictly defining what the AI does versus what humans do. The AI handles heavy-lift tasks (like drafting, summarizing, initial analysis), while humans perform quick reviews, provide raw inputs, or make final judgments. All interactions are designed to be time-boxed and focused. This ensures that the integration of AI yields tangible productivity gains rather than adding more work. The co-management flows have been tuned so that every minute a human spends interacting with the system yields significantly more output (e.g. a 5-minute review of an AI-generated plan might save the team an hour of manual planning work).

**Architectural Assumptions:** The HORIZON Protocol assumes the following about the environment and team:

* The team uses TIP, which provides a central repository of documents (with version control), a library of templates, and an automation engine for LLM-driven workflows. All artifacts (prompts, templates, documents) live in this environment for easy reuse and tracking.
* The AI model is a locally hosted 30-billion-parameter LLM (via Ollama) with no external dependencies. This means all data stays under the team’s control (a plus for privacy/compliance) and the model’s knowledge is limited to what is provided to it (it won’t fetch new info from the internet unless fed via documents). The model has a reasonably large context window (e.g. 4k tokens), but not infinite, so efficient prompt design is crucial.
* Team size is small (1–9 people), which allows for flexibility in role rotation and ensures every member can reasonably touch the AI outputs daily. The process is scalable within this range, but it’s optimized for intimate team collaboration (where communication overhead is low and each person wears multiple hats).
* TIP features like document versioning, linking, and search are leveraged to ensure the AI always has access to up-to-date and relevant context. When a workflow runs, it’s assumed the AI can retrieve the latest content of any referenced document in the prompt context (either automatically or via a prompt to include that content).
* No specialized technical infrastructure or coding is required from end-users; everything happens through TIP’s standard interface (documents, templates, and workflow executions). For example, users trigger or schedule workflows via TIP’s UI, and outputs are saved back into TIP. The design abstracts away complexities like repositories or code scripts, focusing only on TIP’s native capabilities.

**Prompt Design Best Practices (Maximizing usefulness of the LLM):** To get the most out of a model, HORIZON enforces careful prompt engineering in its workflows and templates. Key guidelines include:

* **Role and Context Priming:** Every prompt clearly defines the AI’s role and task at the start (e.g. “You are an AI Planning Assistant who will help draft a daily plan...”). This focuses the model’s persona and makes its responses more relevant. Additionally, provide necessary context documents explicitly within the prompt whenever possible (e.g. prepend a summary of the team’s vision or the relevant goal from the Goals document) so the model isn’t guessing or hallucinating missing info.
* **Chain-of-Thought & Decomposition:** The workflows break complex tasks into smaller steps, sometimes even across multiple prompts if needed. For example, rather than asking the AI to "draft a quarterly strategy and plan and also analyze risks" in one go, the workflow might first prompt the AI to summarize current status and challenges, then in a second step generate the strategy update from that summary. This modular approach plays to the model’s strength in focused reasoning and reduces the chance of it getting confused or going off-topic. In prompts themselves, instruct the model to reason step-by-step when appropriate (“List out the key factors... then based on those, propose...”). This encourages transparent reasoning in the output.
* **Few-Shot Examples:** To compensate for the local model’s lack of fine-tuning on your specific style, incorporate a few-shot technique in prompts where feasible. For instance, within a template prompt, you might include a condensed example of the desired output format or tone (e.g., provide a dummy mini "Yesterday’s Summary" and "Today’s Plan" if generating a daily plan, so the model sees what format to follow). These examples serve as quality benchmarks, mitigating the model’s tendency to produce inconsistent formatting or style. Ensure examples are brief (to conserve context) but illustrative of ideal answers.
* **Avoiding Hallucination:** The prompt should explicitly instruct the AI to use only provided information for factual content. For example: “If a detail is not in the provided context, do not invent it. Either refer to existing plans or state that it’s unknown.” Additionally, any time the AI is asked to analyze or create content that might involve factual data (like market analysis or metrics), it should be given relevant data from TIP documents. By grounding the model in real data, we reduce fabrications. When the AI does propose something imaginative (like a new idea or solution), HORIZON encourages citing which input inspired that (e.g., “Based on Customer Feedback point X, consider doing Y”) to maintain traceability.
* **Conciseness and Clarity:** We instruct the model to be concise and structured in its outputs. Large language models can ramble; to counter this, prompts often include directives like “Limit the response to X bullet points” or “Use clear headings for each section and keep paragraphs brief.” This not only produces cleaner documents directly, but also ensures outputs fit within reading time constraints for the team. A local model might not have the eloquence of GPT-4, but with the right structure it can produce highly usable drafts that only need light editing.
* **Validation and Error-Checking:** While the model can draft content, it sometimes produces irrelevant or duplicative text. HORIZON prompts include checks by design. For example, after generating a list of tasks, the prompt might say: “Check that these tasks do not repeat and are all within scope of today’s goals.” Similarly, for a summary: “Ensure no critical information from the input is omitted.” These self-evaluation hints push the model to double-check its output before finalizing, catching simple mistakes.
* **Iterative Refinement via Human Feedback:** The workflows are set up such that human review is always a step. If the output isn’t satisfactory, the process allows quick iteration: a team member can tweak the prompt or provide an additional nudge (e.g., “the tone is too formal, make it more casual” or “include our product name here”) and rerun. Over time, common adjustments should be folded back into the template prompts. This “prompt tuning” loop is a best practice to gradually improve the model’s outputs for the team’s needs.

**Naming Conventions & Quality Rules:**

* **Templates & Documents:** All official HORIZON templates are prefixed with `HORIZON_` in TIP to denote them as part of this protocol (e.g., `HORIZON_Daily_Log`, `HORIZON_North_Star_Charter`). When instantiated as actual working documents, the `HORIZON_` prefix can be omitted for simplicity. For example, the template `HORIZON_Daily_Log` might generate daily log documents named `Daily_Log_2025-05-18` (with date for uniqueness) in the repository. In general, use ClearTitleCase for document names and include dates or version numbers where relevant (especially for recurring docs like daily or weekly records). This naming convention helps keep documents organized and easily identifiable in TIP’s versioned repository.
* **Workflows & Prompts:** Workflow names should clearly reflect their purpose (e.g., “Daily Log Generator”, “Weekly Review Synthesizer”) without requiring additional explanation. We avoid cryptic names. Within prompt text, important sections or variables are clearly marked (for instance, use markdown headings in the prompt to separate context, instructions, and output sections). Prompts may also use a consistent internal format, e.g., always starting with a brief **Role** definition, then **Context Provided**, then **Task** instructions. This consistency helps any team member quickly understand or modify prompts, and it helps the AI by giving it a familiar structure each time.
* **Document Quality Standards:** Every document produced or updated via HORIZON should meet basic quality checks before being finalized. The protocol encourages a “definition of done” for outputs: e.g., a Daily Plan is only considered complete if it lists all the day’s priorities with owners and deadlines, a Weekly Review must contain highlights, lowlights, and lessons, etc. These criteria can be built into the templates as a checklist. Additionally, formatting rules like “Every document must start with an H1 title and date”, “Use standard bullet style for lists”, “No broken markdown links or images”, are observed to keep the knowledge base tidy and professional. The AI can be prompted to follow many of these automatically (for example, including a header section with date and title in each generated doc). The naming conventions above (consistent titles and date tags) also contribute to quality by ensuring each artifact is distinct and traceable.
* **Version Control & Traceability:** TIP’s version control means every change is tracked. We name and structure prompts so that their function is obvious even when looking at history. For instance, if a prompt template is updated, the commit message might include the template name and change (“Update HORIZON_Weekly_Review prompt for tone”). Similarly, document updates by the AI can include an appended section listing which workflow produced it and when (this could be a small auto-note at the bottom like “*Generated by HORIZON Daily Log workflow on 2025-05-18*”). Such practices create transparency and trust in AI contributions and make it easier to roll back if something goes awry.
* **Clarity & Readability:** Clarity is paramount. Every template and prompt is written in straightforward language, avoiding jargon in instructions to the AI. Likewise, the recommended style for outputs (which the AI is instructed to follow) is clear, concise, and free of fluff. The goal is that any team member or stakeholder reading a HORIZON-generated document can quickly understand it. This means we also enforce length discipline: if an output exceeds a certain length (say a daily log going over one page), the template may encourage the AI to summarize more or move detail to an appendix. By setting these quality guardrails, HORIZON ensures that adding AI doesn’t result in bloated or confusing documents – instead, it produces lean, useful documentation that truly aids the team’s work.
